---
title: "K-Means Clustering - A Sparse Version of Principal Component Analysis"
author: Harish M
output:
  rmdformats::readthedown:
    self_contained: true
    lightbox: true
    gallery: false
    highlight: tango
    code_folding: hide
---

<!-- Custom CSS styles -->
<style>
div.grey {
background-color:#eeeeee;
border-radius: 5px;
padding: 20px;}
</style>

```{r warning = FALSE, message = FALSE, echo = FALSE}
library(tidyverse) # The gasoline for the CaR
library(cluster) # Cluster Analysis
library(factoextra) # Visualizing Clusters
library(plotly) # Interactive Plots
library(gridExtra) # Plot graphs in grids
library(shiny) # Center Plotly charts
library(DT) # Print HTML Tables
library(knitr) # Generating HTML document
library(rmdformats) # Document theme

# Globally controlling code blocks
opts_chunk$set(message = FALSE, 
               warning = FALSE, 
               fig.align = "center",
               fig.height = 4,
               fig.width = 6)
# DT::datatable parameters
options(DT.options = list(paging = FALSE, # disable pagination
                          scrollY = "200px", # enable vertical scrolling
                          scrollX = TRUE, # enable horizontal scrolling
                          scrollCollapse = TRUE,
                          # autoWidth = TRUE, # centering the table output
                          ordering = FALSE, # disable sorting data
                          dom = "t"))  # display just the table
```

# Introduction

K-Means algorithm is one of the popular clustering analysis techniques, that helps
group similar data points into clusters and look for hidden insights. For example,
by analyzing sales data, customers can be grouped into segments to help
understand their behaviour. In one of the <u><a href="https://some.link"target="_blank">earlier posts</a></u>, 
we explored Principal Component Analysis, a dimension reduction tool that transforms 
a number of (possibly) correlated columns into a (smaller) number of uncorrelated 
columns called principal components.

<br>
<div class = "grey"><font size = +1> <i>
K-Means and PCA are thought of as two different procedures having two different
purposes, and at first sight they do not seem related. But that isn't true!
</i> </font></div>
<br>
While principal components can help, for example, <i>rank</i> the records of the 
dataset, k-means just groups the records into, <i>good/bad</i>. Principal components 
are essentially the continuous solutions to the discrete cluster membership 
indicators of K -means clustering<sup><u><a href="http://ranger.uta.edu/~chqding/papers/KmeansPCA1.pdf"target="_blank">[1]
</a></u></sup>. In other words, K-Means is a sparse version of PCA. We explore 
this idea more with the European employment dataset by grouping similar countries. 
The Principal Components are then combined with the results of K-Means to see the 
similarities between the two techniques.

# Required Packages

The packages listed below are required to replicate this project and generate this
HTML document.

* `tidyverse` - The gas for the caR
* `cluster` - Cluster Analysis
* `factoextra` - Visualizing Clusters
* `plotly` - Interactive Plots
* `shiny` - Center Plotly charts
* `gridExtra` - Plot graphs in grids
* `DT` - Print HTML Tables
* `knitr` - Generating HTML document
* `rmdformats` - Document theme

```{r, eval = FALSE}
library(tidyverse) # The gas for the caR
library(cluster) # Cluster Analysis
library(factoextra) # Visualizing Clusters
library(plotly) # Interactive Plots
library(shiny) # Center Plotly charts
library(gridExtra) # Plot graphs in grids
library(DT) # Print HTML Tables
library(knitr) # Generating HTML document
library(rmdformats) # Document theme
```

# Data

The dataset contains the distribution of employment in different sectors of
26 European countries. There are ten variables, the first being the name of
the country, and the remaining nine holding the percentage employment of the
countries' citizens in nine different industries.
```{r}
data <- read.table("https://raw.githubusercontent.com/mr-hn/kmeans-pca/master/european_jobs.txt", 
                   header = TRUE, sep = "\t")

data_definitition <- data.frame(variable = colnames(data),
                                description = c("Name of the country",
                                                "Percentage employed in agriculture",
                                                "Percentage employed in mining",
                                                "Percentage employed in manufacturing",
                                                "Percentage employed in power supply industries",
                                                "Percentage employed in construction",
                                                "Percentage employed in service industries",
                                                "Percentage employed in finance",
                                                "Percentage employed in social and personal services",
                                                "Percentage employed in transport and communications"))
data_definitition %>% datatable(caption = "Variable Descriptions")
```

The dataset seems to contain USSR and East Germany, so I'm assuming this comes
from at least from or before the 80's. Data is read in and a box plot of 
distributions in each industries in presented below.

```{r out.width="100%"}
rownames(data) <- data[,"Country"]
data <- data[,-1]

data %>% datatable(caption = "The Raw Data")

gather(data ,"industry","percentage") %>% 
  plot_ly(x = ~industry, y = ~percentage, type = "box",
          hoverinfo = "text",
          text = ~paste0(rownames(data), " - ", percentage,"%"),
          line = list(color = "rgb(159, 33, 66)"),
          marker = list(color = "rgb(33, 17, 3)")) %>% 
  layout(title = "Distribution of employments in different sectors - Europe, 1980\'s",
         xaxis = list(title = "Industries"),
         yaxis = list(title = "")) %>% 
  config(displayModeBar = F)
```

# K-Means Basics

K-Means clustering is one of the most commonly used unsupervised machine learning
algorithms to group data into a pre-specified `K` clusters. The data points
are assigned to the clusters in such a way that they are as similar to each other as
possible. Each cluster is represented by a <i>centroid</i>, which is a vector of 
mean values of all columns. The within-cluster similarity is measured 
through the sum of squares of the residuals between the centroid means and all
data points assigned to that centroid. For each cluster, 

<center>$$withinness(C) = \sum_{\substack{j\in 1:ncol \\ i\in 1:nrow_{cluster}}}{(x_{ij} - \mu_j)^2} $$</center>

* $C$ is a vector of length `j`, which is the number of variables in the dataset
* $x_{ij}$ is the element belonging to record `i` and column `j`
* $\mu_j$ is the average of the column `j` for all `i` in that cluster
<br>

For reference, this <u><a href="https://uc-r.github.io/kmeans_clustering"target="_blank">excellent post</a></u>
by Brad Boehmke goes into a lot more detail.
<br>

The K-Means algorithm is applied to the dataset with the clusters varying between
2 to 7. The countries are colored based on the cluster they are assigned to and 
presented below on the chloropleth. The number of clusters can be changed from
the dropdown. For two clusters, the algorithm pits Turkey against the rest of the 
Europe. With seven clusters, we see the countries separated broadly into the 
central Europe, the Mediterranean, East and Scandinavia. Turkey and Hungary are
in their own clusters.

```{r out.width=="100", fig.align="center"}
# K-Means requires data to be scaled normally N(0,1).
# This is to make Euclidean distance measurement comparable.
data_scaled <- scale(data) %>% as.data.frame()

# Country code is combined to plot on map
country_code <- read_csv("country_code.csv") %>% 
  rename(country = "COUNTRY", code = "CODE")
data_kmeans <- data_scaled %>% rownames_to_column("country") %>% 
  left_join(country_code, by = "country") 

# Combining the results of K-Means algorithm
data_kmeans$k <- 0
data_kmeans$cluster <- 0
# Into geo_data
geo_data <- NULL

for (i in 2:7) {
  kmeans_model <- kmeans(data_scaled, i, nstart = 25)
  data_kmeans$cluster <- kmeans_model$cluster
  data_kmeans$k <- i
  geo_data <- geo_data %>% bind_rows(data_kmeans)
}

# Drop unnecessary columns
geo_data <- geo_data %>% select(country, code, k, cluster)

# Change data structure for Plotly
geo_data_played <- geo_data %>% spread(k, cluster, sep = "value")
geo_colnames <- colnames(geo_data_played)

# Plotluy Chloropleth settings
l <- list(color = toRGB("grey"), width = 0.5)
g <- list(showframe = FALSE, 
          # lonaxis = list(range = c(-20, 50)),
          # lataxis = list(range = c(30, 90)),
          lonaxis = list(range = c(-10, 50)),
          lataxis = list(range = c(30, 80)),
          showcoastlines = FALSE,
          projection = list(type = 'Mercator'))

# Build plot with data for two clusters visible
geo_plot <- geo_data_played %>% plot_geo() %>%
  add_trace(z = ~kvalue2, color = ~kvalue2, 
            text = ~country, locations = ~code, 
            marker = list(line = l), showscale = FALSE) %>% 
  colorbar(title = "Scale") %>%
  layout(geo = g,
         title = "Countries grouped based on employment distribution",
         annotations = list(x = 0, y = 1, text = "Change clusters in dropdown",
                            showarrow = FALSE, xref = "paper", yref = "paper"))

# Add plots for clusters 3:7 invisible
for (col in geo_colnames[4:8]) {
  geo_plot <- geo_plot %>% 
    add_trace(z = geo_data_played[[col]], color = geo_data_played[[col]], 
              text = ~country, locations = ~code, 
              marker = list(line = l), showscale = FALSE, visible = FALSE) 
}

# Add dropdown for clusters and set corresponding visibility 
geo_plot <- geo_plot %>%
  layout(
    updatemenus = list(
      list(y = 0.9, x = 0.08,
           buttons = list(
             list(method = "restyle",
                  args = list("visible", list(TRUE, FALSE, FALSE, FALSE, FALSE, FALSE)),
                  label = "2"),
             list(method = "restyle",
                  args = list("visible",list(FALSE, TRUE, FALSE, FALSE, FALSE, FALSE)),
                  label = "3"),
             list(method = "restyle",
                  args = list("visible", list(FALSE, FALSE, TRUE, FALSE, FALSE, FALSE)),
                  label = "4"),
             list(method = "restyle",
                  args = list("visible", list(FALSE, FALSE, FALSE, TRUE, FALSE, FALSE)),
                  label = "5"),
             list(method = "restyle",
                  args = list("visible", list(FALSE, FALSE, FALSE, FALSE, TRUE, FALSE)),
                  label = "6"),
             list(method = "restyle",
                  args = list("visible", list(FALSE, FALSE, FALSE, FALSE, FALSE, TRUE)),
                  label = "7")
           )))) %>% 
  config(displayModeBar = FALSE) 

div(geo_plot, align = "center")
```
<font size = -1><i>As we already observed, the dataset contains East and West Germany, 
Yugoslavia and other countries which do not exist in the maps anymore. If you don't
see 7 colors for 7 clusters, that's  because the missing countries are not
being plotted on the map.</i></font><br>

We clearly see patterns emerging that we didn't of before. Countries
that are geographically and culturally closer had similar employment patterns.
Unsupervised learning thus helped us draw inferences without a labelled response
in the input dataset.

## Determining the optimal number of clusters

Because the data was on a map, it was easier for us to say that 7 clusters 
grouped the data better than 2. But in other cases where the output may not be
easily interpretable, it becomes essential to be definite about the number of
clusters. There are three major methods to determine K. <br>

* `Elbow` - The number of clusters is plotted against the within residual sum 
squares and the cluster where there is a sharp drop is considered optimal.  
* `Average Silhouette` - The silhouette value is a measure of how similar a data 
point is to its own cluster compared to other clusters. The optimal number of 
clusters is the one that maximizes the average silhouette.
* `Gap Statistic` - This method computes the within-cluster variations and compares
them against the clusters of another randomly generated reference dataset. The K
that gives the highest difference is considered optimal.

```{r out.width="100%"}
plot1 <- fviz_nbclust(data_scaled, kmeans, method = "wss") +
  ggtitle("By Elbow Method") + xlab("") + theme(text = element_text(size = 8))
plot2 <- fviz_nbclust(data_scaled, kmeans, method = "silhouette") +
  ggtitle("By Silhouette Method") + xlab("") + theme(text = element_text(size = 8))
set.seed(1804)
gap_stat <- clusGap(data_scaled, FUN = kmeans, nstart = 25, K.max = 12, B = 50)
plot3 <- fviz_gap_stat(gap_stat) +
  ggtitle("By Gap Statistic") + xlab("") + theme(text = element_text(size = 8))

grid.arrange(plot1, plot2, plot3, nrow = 2, ncol = 2)
```

The result of the Elbow method is ambiguous, while three clusters have the 
highest average silhouette. The gap statistic method suggests one cluster as
the optimum, which is impractical.<br>

Going by the `Silhouette` method, three clusters broadly separate the data into 
western and eastern Europe, leaving out Turkey and Yugoslavia into the third cluster.
This seems <i>fair,</i> and the rest of the document will work with three clusters.

```{r}
data$cluster <- kmeans(data_scaled, 3, nstart = 25)$cluster %>% as.numeric()

data %>% rownames_to_column("country") %>% 
  ggplot(aes(x = reorder(country, cluster), color = as.factor(cluster))) + 
  geom_text(aes(y = 1),label = rownames(data)) +
  coord_flip() + theme_minimal() +
  theme(legend.position = "none", axis.ticks = element_blank(), axis.text = element_blank(),
        axis.title = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

# Merging Results with Principal Components

PCA model is applied to the data and the results are printed below. It's observed
that the first three components explain ~75% of the variance. We will be utilizing
only the three components to make plotting graphs possible.
```{r}
pca_model <- princomp(data[1:9], cor = TRUE) %>% summary(loadings = TRUE)
eigen_values <- pca_model$sdev^2

pca_model$sdev %>% as.data.frame() %>% rownames_to_column("component") %>% 
  bind_cols(eigen_value = round(sqrt(eigen_values), 3),
            proportion = round(eigen_values/sum(eigen_values), 3),
            cumulative = round(cumsum(eigen_values)/sum(eigen_values), 3)) %>% 
  select(component, eigen_value, proportion, cumulative) %>% 
  datatable(caption = "Eigen Values of Principal Component Analysis")
```

The data points are plotted on a 3-D plane of the first three principal components
and the results from clustering are rendered as colors. The output makes it apparent
how closely PCA and K-Means are related. While, PCA assigns each country to a point 
in the 3-dimensional plane, the role of K-Means is only to assign the country to 
one of the K clusters.

```{r out.width="100%"}
pca_model$scores %>% as.data.frame() %>% rownames_to_column("country") %>% 
  bind_cols(data) %>% select(Comp.1, Comp.2, Comp.3, cluster, country) %>% 
  rename("component_1" = Comp.1, "component_2" = Comp.2, "component_3" = Comp.3) %>% 
  plot_ly(x = ~component_1, y = ~component_2, z = ~component_3, color = ~cluster,
          hoverinfo = "text", mode = "markers",
          text = ~paste(country, "\n",
                        "Component 1 =", round(component_1, 3),"\n",
                        "Component 2 =", round(component_2, 3),"\n",
                        "Component 3 =", round(component_3, 3),"\n")) %>% 
  add_markers() %>% hide_colorbar() %>% 
  layout(scene = list(xaxis = list(title = 'Component 1'),
                      yaxis = list(title = 'Component 2'),
                      zaxis = list(title = 'Component 3'),
                      camera = list(eye = list(x = 1, y = -1, z = 1),
                                    up = list(x = 0, y = 0, z = 1))),
         title = "Data plotted against the three principal components") 
```

# Matrix Algebra and Summary

### For K-Means

K-Means algorithm is essentially an optimization problem where the objective function
is to come up with centroids and cluster assignments such that, the residual 
sum squares are minimized. Following what we saw earlier, this can be defined
in matrix terms as

<br><center>$$minimize\ ||CW - X||^2 $$</center><br>

* C denotes a matrix of mean values of all variables in different clusters. The dimension
 is `J` by `K` where J is the number of variables in the dataset and K is the number 
of clusters. Thus each column of matrix C represents a cluster.
* W is the assignment matrix of dimension `K` rows and `I` columns where I is
the number of records in the dataset.  
* X is the original data. The double vertical bar in the equation denotes the 
Euclidean norm, which is root sum squares of the elements of the matrix.

The figure below, from page 293 of the book<u><a href="https://docs.wixstatic.com/ugd/f09e45_6f190e621c5c48e48f67f99a50633ce0.pdf"
target="_blank">Machine Learning Redifined</a></u> illustrates the concept.
![](https://raw.githubusercontent.com/mr-hn/kmeans-pca/master/MLRedefined.png)

The point of note in the above figure is the matrix `W`. The constraint for solving
the optimization problem of W is that each column should have one `1` and all other
elements of the column must be `0`. That is, each point should belong to only one
cluster.
<br>

### For PCA

PCA solves the same objective function, with the only difference being that the
"categorical" constraint on W is removed. It's the relaxed version of K-Means
problem where

* `C` is a matrix of principal components and
* `W` is the matrix of eigen vectors, transposed

The optimization solves for C and W to minimize the difference between C*W and X.

```{r eval = FALSE}
# Data matrixed and scaled. PCA models always scale data
x <- data[,1:9] %>% as.matrix() %>% scale()
# Principal Component scores 
c <- pca_model$scores %>% as.data.frame() %>% as.matrix()
# Eigen vectors
w <- pca_model$loadings[1:9,1:9] %>% as.data.frame() %>%  as.matrix()



# Original data * Eigen vector - Principal Components = 0
(x %*% w) - c

# Origiganl Data - Principal Components * Inverse(Eigen vector) = 0
x - (c %*% t(w))
# Eigen vector is orthogonal, hence inverse(w) = transpose(w)
```

### For Linear Regression

Come to think of it, the same minimization can be applied to <u><a href="http://mr-hn.github.io/classificationTech"target="_blank">generalized linear 
regression</a></u> as well. The matrices in that case would be

* `C` - The response values Y
* `W` - The coefficients to be determined

### Summary

Principal Component Analysis is one of the classic dimension reduction algorithms.
While not particularly useful for predictive modeling, the fundamental matrix
minimization problem helps to frame our understanding of other popular models
such as K-Means and Linear Regression. Just goes to prove the importance of underlying
concepts in data science.

<u><a href="https://www.linkedin.com/in/harish-morekonda/"target="_blank">LinkedIn</a></u>

Here's a link to the repository - <u><a href="https://github.com/mr-hn/kmeans-pca"target="_blank">
GitHub</a></u>